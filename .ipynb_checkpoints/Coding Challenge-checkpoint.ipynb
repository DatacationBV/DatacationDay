{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to the Datacation Bootcamp!\n",
    "\n",
    "Today, we present a coding challenge to you.\n",
    "In this challenge, you will be given two brain tumor dataset. A train and a test dataset.\n",
    "The target variable has been removed from the test dataset. You will try different Machine Learning models and\n",
    "use your best model to predict whether patients have a brain tumor or not.\n",
    "\n",
    "Try to get as far as possible in the following exercises, increasing in difficulty:\n",
    "\n",
    "1. Loading the training set train_brain.csv and split the training set in a training and validation set, then preprocess the data. Options include:\n",
    "    - imputing missing values\n",
    "    - one-hot-encoding categorical values\n",
    "    - scaling the data\n",
    "2. Implement the machine learning model called the Support Vector Machine (SVM) and optimize based on the validation accuracy.\n",
    "3. Visualize the SVM accuracy results in a graph.\n",
    "4. Use a grid search to find the optimal hyperparameters of the SVM, KNN and RandomForest models using 3-fold cross validation.\n",
    "5. Visualize the SVM, KNN and RandomForest accuracy results in a heatmap.\n",
    "6. Implement a Neural Network and visualize the loss and accuracy, both for the test and training dataset.\n",
    "7. Apply any type of model and preprocessing steps necessary to achieve the highest possible validation accuracy.\n",
    "8. Use the test_brain.csv dataset to predict whether the patients have a brain tumor or not. The target variable has been removed from the dataset.\n",
    "   Save the prediction results in a list with the same order as the patients in the test dataset.\n",
    "   Use the given code to store the list as .pkl file and save it using your group number.\n",
    "   Finally, do a push request to the github repository. We will calculate your final accuracy score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              1          2          4          5          6          7  \\\n",
      "0      1.250042  19.145029   9.584353  34.984717   7.027265   6.689411   \n",
      "1      2.970214  27.764929  20.236078  38.657649  10.392102  11.340814   \n",
      "2      3.135308  32.574846  10.915899  32.318764   6.779047   7.626425   \n",
      "3      4.277058  47.738345  16.489040  39.429322   6.935775  10.205494   \n",
      "4      4.473978  16.115129  10.403431  38.915410   7.852578   6.757196   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "48397  5.097584  16.073367  22.228708  34.273965  10.838109   6.613083   \n",
      "48398  2.479135  33.375509  16.584398  27.859037   9.112503  15.142838   \n",
      "48399       NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "48400  7.863778  41.672625  19.498322  37.716725  11.986951  11.115559   \n",
      "48401  9.408801  22.779617  11.257911  37.299801  10.411007   8.616008   \n",
      "\n",
      "               8         9         10         11  ...         92         93  \\\n",
      "0      36.111964  4.899921  23.910874  15.108962  ...  45.801942   7.525531   \n",
      "1      47.482511  5.879906  24.805936  17.477360  ...  57.856163   9.533020   \n",
      "2      35.991821  3.919938  24.728131  14.874960  ...  34.390765   6.145599   \n",
      "3      38.813908  5.798239  26.308526  11.595322  ...  45.782280   9.026123   \n",
      "4      23.161443  5.879904  23.663073  16.855977  ...  49.892015   8.286800   \n",
      "...          ...       ...        ...        ...  ...        ...        ...   \n",
      "48397  31.836607  4.899924  25.022205  12.572187  ...  52.860377   9.100752   \n",
      "48398  50.433874  4.899923  25.441912  10.501661  ...  41.352104  12.593943   \n",
      "48399        NaN  7.758212  23.269080        NaN  ...  57.179680        NaN   \n",
      "48400  44.826977  1.469974  22.871764  11.880953  ...  59.713279   9.405147   \n",
      "48401  31.470735  4.899922  24.732553  17.692858  ...  31.212530   6.762669   \n",
      "\n",
      "              94         95        97         98  99        100       101  \\\n",
      "0      13.317211  26.600325  5.774739   3.040639   0  11.309413  0.995232   \n",
      "1       4.681385  44.953855  5.930394   8.292149   0   2.363188  6.675311   \n",
      "2      12.953741  28.016364  6.049601  10.440941   0   9.620541  2.573866   \n",
      "3       7.586292  29.856851  5.166861  13.376133   1   4.444706  5.679290   \n",
      "4       9.337974  17.419744  6.444625   6.748303   0   8.737056  4.564449   \n",
      "...          ...        ...       ...        ...  ..        ...       ...   \n",
      "48397   9.309299  19.530022  4.856144  25.487932   0   8.846477  5.248273   \n",
      "48398   6.964221  42.364456  6.405751  13.065721   0   2.377983  4.146786   \n",
      "48399        NaN        NaN       NaN        NaN   0        NaN       NaN   \n",
      "48400  13.864822  36.278583  5.749332  25.025941   0   5.908571  7.211804   \n",
      "48401   8.567103  19.568341  6.272767   1.682811   0   8.449075  7.784765   \n",
      "\n",
      "       target  \n",
      "0           0  \n",
      "1           0  \n",
      "2           1  \n",
      "3           0  \n",
      "4           1  \n",
      "...       ...  \n",
      "48397       1  \n",
      "48398       0  \n",
      "48399       0  \n",
      "48400       0  \n",
      "48401       0  \n",
      "\n",
      "[48402 rows x 85 columns]\n"
     ]
    }
   ],
   "source": [
    "######################################   EXERCISE 1   ######################################\n",
    "'''\n",
    "Loading the training set train_brain.csv and split the training set in a training and validation set, then preprocess the data. Options include:\n",
    "    - imputing missing values\n",
    "    - one-hot-encoding categorical values\n",
    "    - scaling the data\n",
    "'''\n",
    "\n",
    "# Read in the dataset: (https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "data = pd.read_csv('train_brain.csv', na_values='na')\n",
    "data = data._get_numeric_data()\n",
    "print(data)\n",
    "\n",
    "# Split the training set in a train and validation set, use random_state = 0: (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "# Impute missing values: (https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)\n",
    "\n",
    "# Scale numerical columns: (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.head(data,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(data.iloc[:,:-1], data.iloc[:,-1:], test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "\n",
    "X_train_new = imp.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# We dont want to learn about test data, only change it according to previously learnt information\n",
    "X_test_new = imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target\n",
      "1       0\n",
      "3       0\n",
      "4       1\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\datacation\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:38:26] WARNING: D:\\bld\\xgboost-split_1634712635879\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[14:38:26] WARNING: D:\\bld\\xgboost-split_1634712635879\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\datacation\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan]\n",
      "  category=UserWarning,\n",
      "D:\\anaconda3\\envs\\datacation\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "D:\\anaconda3\\envs\\datacation\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\anaconda3\\envs\\datacation\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=<generator object _BaseKFold.split at 0x000001C7895B3D48>,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           enable_categorical=False, gamma=None,\n",
       "                                           gpu_id=None, importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=0.02,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           mi...\n",
       "                                           reg_alpha=None, reg_lambda=None,\n",
       "                                           scale_pos_weight=None, silent=True,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   n_iter=5, n_jobs=4,\n",
       "                   param_distributions={'colsample_bytree': [0.6, 0.8, 1.0],\n",
       "                                        'gamma': [0.5, 1, 1.5, 2, 5],\n",
       "                                        'max_depth': [3, 4, 5],\n",
       "                                        'min_child_weight': [1, 5, 10],\n",
       "                                        'subsample': [0.6, 0.8, 1.0]},\n",
       "                   random_state=1001, scoring='roc_auc', verbose=3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "\n",
    "xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n",
    "                    silent=True, nthread=1)\n",
    "folds = 2\n",
    "param_comb = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train_new,y_train), verbose=3, random_state=1001 )\n",
    "\n",
    "random_search.fit(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "   target\n",
      "2       1\n",
      "0       0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "y_test_pred = random_search.predict_proba(X_test)\n",
    "print(y_test_pred)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImplement the machine learning model called the Support Vector Machine (SVM) and optimize based on the validation accuracy.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################   EXERCISE 2   ######################################\n",
    "'''\n",
    "Implement the machine learning model called the Support Vector Machine (SVM) and optimize based on the validation accuracy.\n",
    "'''\n",
    "\n",
    "# Implement the Support Vector Machine: (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nVisualize the SVM accuracy results in a graph.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################   EXERCISE 3   ######################################\n",
    "'''\n",
    "Visualize the SVM accuracy results in a graph.\n",
    "'''\n",
    "\n",
    "# Visualize the SVM results: (https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUse a grid search to find the optimal hyperparameters of the SVM, KNN and RandomForest models using 3-fold cross validation.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################   EXERCISE 4   ######################################\n",
    "'''\n",
    "Use a grid search to find the optimal hyperparameters of the SVM, KNN and RandomForest models using 3-fold cross validation.\n",
    "'''\n",
    "\n",
    "# Make use of GridSearchCV: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "# KNN: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "# RandomForest: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nVisualize the SVM, KNN and RandomForest accuracy results in a heatmap.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################   EXERCISE 5   ######################################\n",
    "'''\n",
    "Visualize the SVM, KNN and RandomForest accuracy results in a heatmap.\n",
    "'''\n",
    "\n",
    "# Make us of a heatmap: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImplement a basic Neural Network and visualize the loss and accuracy, both for the test and training dataset.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################   EXERCISE 6   ######################################\n",
    "'''\n",
    "Implement a basic Neural Network and visualize the loss and accuracy, both for the test and training dataset.\n",
    "'''\n",
    "\n",
    "# Make use of the MLPClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nApply any type of model and preprocessing steps necessary to achieve the highest possible validation accuracy.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################   EXERCISE 7   ######################################\n",
    "'''\n",
    "Apply any type of model and preprocessing steps necessary to achieve the highest possible validation accuracy.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################   EXERCISE 8   ######################################\n",
    "'''\n",
    "Time to predict using your best ML model!\n",
    "Use the test_brain.csv dataset to predict whether the patients have a brain tumor or not. The target variable has been removed from the dataset.\n",
    "Save the prediction results in a list with the same order as the patients in the test dataset.\n",
    "Use the below given code to store the list as .pkl file and save it using your group number.\n",
    "Finally, do a push request to the github repository. We will calculate your final accuracy score.\n",
    "'''\n",
    "\n",
    "import pickle\n",
    "\n",
    "group_number = 9\n",
    "ypred = []\n",
    "\n",
    "with open(f'test_predictions_group_{group_number}.pkl', 'wb') as f:\n",
    "    pickle.dump(ypred, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
